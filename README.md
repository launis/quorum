# Cognitive Quorum - Dynamic Workflow Engine

**Cognitive Quorum** is an advanced, data-driven workflow engine designed for complex, multi-agent AI assessments. It orchestrates interactions between Large Language Models (LLMs) and deterministic Python code (Hooks) to produce high-quality, verifiable, and transparent results.

## ğŸš€ Key Features

*   **Generic Workflow Engine**: A single, agnostic engine executes any workflow defined in the database. No hardcoded agent logic.
*   **Hybrid Architecture**: Seamlessly combines LLMs (e.g., Google Gemini, OpenAI GPT-4) for reasoning and generation with deterministic Python Hooks for tasks like calculations, data parsing, and external API calls.
*   **Data-Driven Configuration**: Workflows, prompts, execution steps, and business rules are defined as data in a document database, seeded from modular JSON fragments and Jinja2 templates. This allows for rapid iteration and modification without code changes.
*   **Dynamic Registries**: Utilizes a registry pattern for dynamically mapping string identifiers to Pydantic schemas (`SchemaRegistry`) and Python functions (`HookRegistry`), enabling flexible and extensible design.
*   **Reliability & Robustness**:
    *   **UTF-8 Support**: Ensures full compatibility with international character sets across the entire stack.
    *   **Error Fallbacks**: Implements automatic retry logic and model switching (e.g., from a faster to a more powerful model on failure) to enhance resilience.
*   **Adversarial Testing**: Includes a comprehensive test suite for simulating rule violations (e.g., Prompt Injection, PII Leaks) and technical failures.
*   **External Integrations**: Supports connections to external services like the Google Custom Search API for real-time fact-checking and provides a framework for Retrieval-Augmented Generation (RAG).
*   **Transparency & Explainability (XAI)**: Designed for full traceability of inputs, outputs, and data sources. Capable of generating detailed XAI reports with uncertainty quantification.

## ğŸ—ï¸ System Architecture

The system is built on a modern, service-oriented architecture, designed for scalability and maintainability. The core logic is encapsulated in the `src/` directory, promoting a clean separation of concerns.

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                # FastAPI routers and server logic
â”‚   â”œâ”€â”€ components/         # Core building blocks like Hooks and Registries
â”‚   â”‚   â””â”€â”€ hooks/          # Deterministic Python functions (e.g., search, parsing)
â”‚   â”œâ”€â”€ database/           # Database clients and adapters (TinyDB, Firestore)
â”‚   â”œâ”€â”€ engine/             # The main workflow orchestration and execution logic
â”‚   â”‚   â”œâ”€â”€ orchestrator.py # Manages the overall workflow execution
â”‚   â”‚   â””â”€â”€ executor.py     # Executes individual steps (LLM calls or Hook calls)
â”‚   â”œâ”€â”€ models/             # Pydantic models and schema definitions
â”‚   â””â”€â”€ main.py             # Application entry point
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ db.json             # Default local database (TinyDB)
â”‚   â”œâ”€â”€ fragments/          # Reusable JSON components for building workflows
â”‚   â””â”€â”€ templates/          # Jinja2 templates for generating prompts
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ scenarios/         # Test data for various scenarios
â”‚   â””â”€â”€ test_*.py           # Pytest integration and unit tests
â”œâ”€â”€ scripts/                # Helper and utility scripts (e.g., database seeding)
â”œâ”€â”€ docs/                   # Project documentation
â”œâ”€â”€ config.py               # Centralized configuration
â”œâ”€â”€ docker-compose.yml      # Docker Compose for local deployment
â”œâ”€â”€ requirements.txt        # Python package dependencies
â””â”€â”€ README.md
```

## âš™ï¸ Getting Started

### Prerequisites

*   Python 3.10+
*   Docker and Docker Compose
*   API keys for required services (e.g., Google AI, OpenAI, Google Custom Search)

### Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-repo/cognitive-quorum.git
    cd cognitive-quorum
    ```

2.  **Install Python dependencies:**
    ```bash
# Cognitive Quorum - Dynamic Workflow Engine

**Cognitive Quorum** is an advanced, data-driven workflow engine designed for complex, multi-agent AI assessments. It orchestrates interactions between Large Language Models (LLMs) and deterministic Python code (Hooks) to produce high-quality, verifiable, and transparent results.

## ğŸš€ Key Features

*   **Generic Workflow Engine**: A single, agnostic engine executes any workflow defined in the database. No hardcoded agent logic.
*   **Hybrid Architecture**: Seamlessly combines LLMs (e.g., Google Gemini, OpenAI GPT-4) for reasoning and generation with deterministic Python Hooks for tasks like calculations, data parsing, and external API calls.
*   **Data-Driven Configuration**: Workflows, prompts, execution steps, and business rules are defined as data in a document database, seeded from modular JSON fragments and Jinja2 templates. This allows for rapid iteration and modification without code changes.
*   **Dynamic Registries**: Utilizes a registry pattern for dynamically mapping string identifiers to Pydantic schemas (`SchemaRegistry`) and Python functions (`HookRegistry`), enabling flexible and extensible design.
*   **Reliability & Robustness**:
    *   **UTF-8 Support**: Ensures full compatibility with international character sets across the entire stack.
    *   **Error Fallbacks**: Implements automatic retry logic and model switching (e.g., from a faster to a more powerful model on failure) to enhance resilience.
*   **Adversarial Testing**: Includes a comprehensive test suite for simulating rule violations (e.g., Prompt Injection, PII Leaks) and technical failures.
*   **External Integrations**: Supports connections to external services like the Google Custom Search API for real-time fact-checking and provides a framework for Retrieval-Augmented Generation (RAG).
*   **Transparency & Explainability (XAI)**: Designed for full traceability of inputs, outputs, and data sources. Capable of generating detailed XAI reports with uncertainty quantification.

## ğŸ—ï¸ System Architecture

The system is built on a modern, service-oriented architecture, designed for scalability and maintainability. The core logic is encapsulated in the `src/` directory, promoting a clean separation of concerns.

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                # FastAPI routers and server logic
â”‚   â”œâ”€â”€ components/         # Core building blocks like Hooks and Registries
â”‚   â”‚   â””â”€â”€ hooks/          # Deterministic Python functions (e.g., search, parsing)
â”‚   â”œâ”€â”€ database/           # Database clients and adapters (TinyDB, Firestore)
â”‚   â”œâ”€â”€ engine/             # The main workflow orchestration and execution logic
â”‚   â”‚   â”œâ”€â”€ orchestrator.py # Manages the overall workflow execution
â”‚   â”‚   â””â”€â”€ executor.py     # Executes individual steps (LLM calls or Hook calls)
â”‚   â”œâ”€â”€ models/             # Pydantic models and schema definitions
â”‚   â””â”€â”€ main.py             # Application entry point
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ db.json             # Default local database (TinyDB)
â”‚   â”œâ”€â”€ fragments/          # Reusable JSON components for building workflows
â”‚   â””â”€â”€ templates/          # Jinja2 templates for generating prompts
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ scenarios/         # Test data for various scenarios
â”‚   â””â”€â”€ test_*.py           # Pytest integration and unit tests
â”œâ”€â”€ scripts/                # Helper and utility scripts (e.g., database seeding)
â”œâ”€â”€ docs/                   # Project documentation
â”œâ”€â”€ config.py               # Centralized configuration
â”œâ”€â”€ docker-compose.yml      # Docker Compose for local deployment
â”œâ”€â”€ requirements.txt        # Python package dependencies
â””â”€â”€ README.md
```

## âš™ï¸ Getting Started

### Prerequisites

*   Python 3.10+
*   Docker and Docker Compose
*   API keys for required services (e.g., Google AI, OpenAI, Google Custom Search)

### Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-repo/cognitive-quorum.git
    cd cognitive-quorum
    ```

2.  **Install Python dependencies:**
# Cognitive Quorum - Dynamic Workflow Engine

**Cognitive Quorum** is an advanced, data-driven workflow engine designed for complex, multi-agent AI assessments. It orchestrates interactions between Large Language Models (LLMs) and deterministic Python code (Hooks) to produce high-quality, verifiable, and transparent results.

## ğŸš€ Key Features

*   **Generic Workflow Engine**: A single, agnostic engine executes any workflow defined in the database. No hardcoded agent logic.
*   **Hybrid Architecture**: Seamlessly combines LLMs (e.g., Google Gemini, OpenAI GPT-4) for reasoning and generation with deterministic Python Hooks for tasks like calculations, data parsing, and external API calls.
*   **Data-Driven Configuration**: Workflows, prompts, execution steps, and business rules are defined as data in a document database, seeded from modular JSON fragments and Jinja2 templates. This allows for rapid iteration and modification without code changes.
*   **Dynamic Registries**: Utilizes a registry pattern for dynamically mapping string identifiers to Pydantic schemas (`SchemaRegistry`) and Python functions (`HookRegistry`), enabling flexible and extensible design.
*   **Reliability & Robustness**:
    *   **UTF-8 Support**: Ensures full compatibility with international character sets across the entire stack.
    *   **Error Fallbacks**: Implements automatic retry logic and model switching (e.g., from a faster to a more powerful model on failure) to enhance resilience.
*   **Adversarial Testing**: Includes a comprehensive test suite for simulating rule violations (e.g., Prompt Injection, PII Leaks) and technical failures.
*   **External Integrations**: Supports connections to external services like the Google Custom Search API for real-time fact-checking and provides a framework for Retrieval-Augmented Generation (RAG).
*   **Transparency & Explainability (XAI)**: Designed for full traceability of inputs, outputs, and data sources. Capable of generating detailed XAI reports with uncertainty quantification.

## ğŸ—ï¸ System Architecture

The system is built on a modern, service-oriented architecture, designed for scalability and maintainability. The core logic is encapsulated in the `src/` directory, promoting a clean separation of concerns.

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                # FastAPI routers and server logic
â”‚   â”œâ”€â”€ components/         # Core building blocks like Hooks and Registries
â”‚   â”‚   â””â”€â”€ hooks/          # Deterministic Python functions (e.g., search, parsing)
â”‚   â”œâ”€â”€ database/           # Database clients and adapters (TinyDB, Firestore)
â”‚   â”œâ”€â”€ engine/             # The main workflow orchestration and execution logic
â”‚   â”‚   â”œâ”€â”€ orchestrator.py # Manages the overall workflow execution
â”‚   â”‚   â””â”€â”€ executor.py     # Executes individual steps (LLM calls or Hook calls)
â”‚   â”œâ”€â”€ models/             # Pydantic models and schema definitions
â”‚   â””â”€â”€ main.py             # Application entry point
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ db.json             # Default local database (TinyDB)
â”‚   â”œâ”€â”€ fragments/          # Reusable JSON components for building workflows
â”‚   â””â”€â”€ templates/          # Jinja2 templates for generating prompts
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ scenarios/         # Test data for various scenarios
â”‚   â””â”€â”€ test_*.py           # Pytest integration and unit tests
â”œâ”€â”€ scripts/                # Helper and utility scripts (e.g., database seeding)
â”œâ”€â”€ docs/                   # Project documentation
â”œâ”€â”€ config.py               # Centralized configuration
â”œâ”€â”€ docker-compose.yml      # Docker Compose for local deployment
â”œâ”€â”€ requirements.txt        # Python package dependencies
â””â”€â”€ README.md
```

## âš™ï¸ Getting Started

### Prerequisites

*   Python 3.10+
*   Docker and Docker Compose
*   API keys for required services (e.g., Google AI, OpenAI, Google Custom Search)

### Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-repo/cognitive-quorum.git
    cd cognitive-quorum
    ```

2.  **Install Python dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure environment variables:**
    Create a `.env` file in the root directory and add your API keys. Refer to `config.py` for the required variable names.
    ```env
    GOOGLE_API_KEY="your_google_api_key"
    OPENAI_API_KEY="your_openai_api_key"
    GOOGLE_SEARCH_API_KEY="your_search_api_key"
    GOOGLE_SEARCH_CX="your_search_cx"
    ```

## ğŸ§ª Running Tests

To ensure the system is functioning correctly, run the test suite:
```bash
pytest
```

## ğŸ“– Viewing Documentation

**Option 1: Online**
[View GitHub Pages](https://launis.github.io/quorum/)

**Option 2: Local Server**
```bash
mkdocs serve -a localhost:8001
# Open http://localhost:8001
```